import os
import telebot
import requests
import re
import random
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS

BOT_TOKEN = "8546746980:AAF3z5K85WaBMC-SKTSTN5Tx_dXxXyZXIoQ"
NEWS_API_KEY = "E16b35592a2147989d80d46457d4f916" 
CHANNEL_ID = "@SUP_V_BotK"
DB_FILE = "last_links.txt"

bot = telebot.TeleBot(BOT_TOKEN)

def get_posted_data():
    if not os.path.exists(DB_FILE): return set()
    with open(DB_FILE, "r", encoding="utf-8") as f:
        return set(f.read().splitlines())

def save_posted_data(link, title):
    clean_title = re.sub(r'[^\w\s]', '', title).lower().strip()
    with open(DB_FILE, "a", encoding="utf-8") as f:
        f.write(f"{link}\n{clean_title}\n")

def get_full_article(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        for s in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'form']): s.decompose()
        paragraphs = soup.find_all('p')
        text = " ".join([p.get_text() for p in paragraphs if len(p.get_text()) > 50])
        return text[:3500] if text else None
    except:
        return None

def rewrite_text(title, content):
    prompt = (
        f"–ò–ù–°–¢–†–£–ö–¶–ò–Ø: –ù–∞–ø–∏—à–∏ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π, –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –ø–æ—Å—Ç –¥–ª—è Telegram. "
        f"–ò—Å–ø–æ–ª—å–∑—É–π HTML-—Ä–∞–∑–º–µ—Ç–∫—É (<b> –∏ <i>).\n\n"
        f"–°–¢–†–£–ö–¢–£–†–ê:\n"
        f"1. –ñ–∏—Ä–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫.\n"
        f"2. –°—É—Ç—å —Å–æ–±—ã—Ç–∏—è (2-3 –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –∞–±–∑–∞—Ü–∞).\n"
        f"3. –°–ø–∏—Å–æ–∫ —Ñ–∞–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ ‚Ä¢.\n"
        f"4. –ë–ª–æ–∫ '–ò—Ç–æ–≥'.\n"
        f"5. –•–µ—à—Ç–µ–≥–∏.\n\n"
        f"–ó–ê–ü–†–ï–¢: –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –≤–≤–æ–¥–Ω—ã–µ —Ñ—Ä–∞–∑—ã. –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–º.\n\n"
        f"–ó–ê–ì–û–õ–û–í–û–ö: {title}\n"
        f"–î–ê–ù–ù–´–ï: {content[:2500]}"
    )
    try:
        with DDGS() as ddgs:
            response = ddgs.chat(prompt, model='claude-3-haiku')
            if not response: return None
            text = response.strip()
            text = re.sub(r'^(–í–æ—Ç|–í–∞—à|–î–µ—Ä–∂–∏—Ç–µ|–ì–æ—Ç–æ–≤—ã–π|–ö–æ–Ω–µ—á–Ω–æ|–†–µ–¥–∞–∫—Ç–æ—Ä).*:(\s+)?', '', text, flags=re.IGNORECASE | re.MULTILINE)
            return text
    except:
        return None

def run():
    url = f"https://newsapi.org/v2/everything?q=(IT OR —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ OR –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ OR –≥–∞–¥–∂–µ—Ç—ã)&language=ru&sortBy=publishedAt&apiKey={NEWS_API_KEY}"
    try:
        res = requests.get(url)
        articles = res.json().get('articles', [])
    except: return

    if not articles: return

    posted_data = get_posted_data()
    random.shuffle(articles)

    for art in articles:
        link = art['url']
        title = art['title']
        clean_title = re.sub(r'[^\w\s]', '', title).lower().strip()
        
        if link in posted_data or clean_title in posted_data: continue
        
        raw_text = get_full_article(link)
        content = raw_text if (raw_text and len(raw_text) > 400) else art.get('description', "")
        
        if not content or len(content) < 200: continue

        final_post = rewrite_text(title, content)
        
        if not final_post or len(final_post) < 400:
            continue

        caption = f"{final_post}\n\nüóû <b>–ü–æ–¥–ø–∏—à–∏—Å—å –Ω–∞ <a href='https://t.me/SUP_V_BotK'>SUP_V_BotK</a></b>"
        
        try:
            if art.get('urlToImage'):
                bot.send_photo(CHANNEL_ID, art['urlToImage'], caption=caption, parse_mode='HTML')
            else:
                bot.send_message(CHANNEL_ID, caption, parse_mode='HTML')
            save_posted_data(link, title)
            break 
        except:
            continue

if __name__ == "__main__":
    run()
