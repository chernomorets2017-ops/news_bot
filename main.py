import os
import telebot
import requests
import re
import random
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS

BOT_TOKEN = "8546746980:AAF3z5K85WaBMC-SKTSTN5Tx_dXxXyZXIoQ"
NEWS_API_KEY = "E16b35592a2147989d80d46457d4f916" 
CHANNEL_ID = "@SUP_V_BotK"
DB_FILE = "last_links.txt"

bot = telebot.TeleBot(BOT_TOKEN)

def get_posted_data():
    if not os.path.exists(DB_FILE): return set()
    with open(DB_FILE, "r", encoding="utf-8") as f:
        return set(f.read().splitlines())

def save_posted_data(link, title):
    clean_title = re.sub(r'[^\w\s]', '', title).lower().strip()
    with open(DB_FILE, "a", encoding="utf-8") as f:
        f.write(f"{link}\n{clean_title}\n")

def get_full_article(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        for s in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']): s.decompose()
        return " ".join([p.get_text() for p in soup.find_all('p')])[:2500]
    except:
        return None

def rewrite_text(title, content):
    system_prompt = (
        "–¢—ã ‚Äî –¥–µ—Ä–∑–∫–∏–π —Ä–µ–¥–∞–∫—Ç–æ—Ä –º–æ–ª–æ–¥–µ–∂–Ω–æ–≥–æ –°–ú–ò. –¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ø–µ—Ä–µ—Å–∫–∞–∑–∞—Ç—å –Ω–æ–≤–æ—Å—Ç—å –°–í–û–ò–ú–ò —Å–ª–æ–≤–∞–º–∏.\n"
        "–ó–ê–ü–†–ï–©–ï–ù–û: –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞.\n"
        "–ù–£–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–µ–Ω–≥, –∏—Ä–æ–Ω–∏—é –∏ –¥–µ–ª–∞—Ç—å —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–º.\n\n"
        f"–ù–û–í–û–°–¢–¨: {title}\n"
        f"–¢–ï–ö–°–¢: {content[:1500]}\n\n"
        "–°–¢–†–£–ö–¢–£–†–ê:\n"
        "1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–ø—Å–æ–º —Å —ç–º–æ–¥–∑–∏.\n"
        "2. –°—É—Ç—å –≤ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö (–ø–µ—Ä–µ—Å–∫–∞–∑).\n"
        "3. 3 –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∞–∫—Ç–∞ (–±—É–ª–ª–∏—Ç—ã ‚Ä¢).\n"
        "4. –í–æ–ø—Ä–æ—Å –≤ –∫–æ–Ω—Ü–µ.\n"
        "5. 3 —Ö–µ—à—Ç–µ–≥–∞."
    )
    try:
        with DDGS() as ddgs:
            response = ddgs.chat(system_prompt, model='gpt-4o-mini')
            text = response.strip()
            # –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä, –µ—Å–ª–∏ –Ω–µ–π—Ä–æ–Ω–∫–∞ –Ω–∞—á–Ω–µ—Ç –≤–µ–∂–ª–∏–≤–æ—Å—Ç—å –≤–∫–ª—é—á–∞—Ç—å
            text = re.sub(r'^(–í–æ—Ç|–ö–æ–Ω–µ—á–Ω–æ|–î–µ—Ä–∂–∏|–†–µ–¥–∞–∫—Ç–æ—Ä).*:(\s+)?', '', text, flags=re.IGNORECASE)
            return text
    except:
        return None

def run():
    # –†–∞—Å—à–∏—Ä–∏–ª —Ç–µ–º—ã, —á—Ç–æ–±—ã –Ω–æ–≤–æ—Å—Ç–∏ –≤—Å–µ–≥–¥–∞ –±—ã–ª–∏
    queries = ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "–±–ª–æ–≥–µ—Ä—ã", "—Å–∫–∞–Ω–¥–∞–ª—ã", "–≥–∞–¥–∂–µ—Ç—ã"]
    q = random.choice(queries)
    url = f"https://newsapi.org/v2/everything?q={q}&language=ru&sortBy=publishedAt&apiKey={NEWS_API_KEY}"
    
    try:
        r = requests.get(url)
        articles = r.json().get('articles', [])
    except: return

    posted_data = get_posted_data()
    random.shuffle(articles)

    for art in articles:
        link, title = art['url'], art['title']
        clean_title = re.sub(r'[^\w\s]', '', title).lower().strip()
        
        if link in posted_data or clean_title in posted_data: continue
        
        raw_text = get_full_article(link)
        content = raw_text if (raw_text and len(raw_text) > 300) else art.get('description', "")
        if not content: continue

        final_post = rewrite_text(title, content)
        if not final_post or len(final_post) < 150: continue

        caption = f"{final_post}\n\nüóû <b>–ü–æ–¥–ø–∏—à–∏—Å—å –Ω–∞ <a href='https://t.me/SUP_V_BotK'>SUP_V_BotK</a></b>"
        
        try:
            if art.get('urlToImage'):
                bot.send_photo(CHANNEL_ID, art['urlToImage'], caption=caption[:1024], parse_mode='HTML')
            else:
                bot.send_message(CHANNEL_ID, caption, parse_mode='HTML')
            save_posted_data(link, title)
            break
        except: continue

if __name__ == "__main__":
    run()
