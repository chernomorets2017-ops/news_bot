import os
import telebot
import requests
import re
import random
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS

BOT_TOKEN = "8546746980:AAF3z5K85WaBMC-SKTSTN5Tx_dXxXyZXIoQ"
NEWS_API_KEY = "E16b35592a2147989d80d46457d4f916" 
CHANNEL_ID = "@SUP_V_BotK"
DB_FILE = "last_links.txt"

bot = telebot.TeleBot(BOT_TOKEN)

def get_posted_data():
    if not os.path.exists(DB_FILE): return set()
    with open(DB_FILE, "r", encoding="utf-8") as f:
        return set(f.read().splitlines())

def save_posted_data(link, title):
    clean_title = re.sub(r'[^\w\s]', '', title).lower().strip()
    with open(DB_FILE, "a", encoding="utf-8") as f:
        f.write(f"{link}\n{clean_title}\n")

def get_full_article(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        for s in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']): s.decompose()
        text = " ".join([p.get_text() for p in soup.find_all('p')])
        return text[:2500]
    except:
        return None

def rewrite_text(title, content):
    prompt = (
        f"SYSTEM: –¢—ã ‚Äî —Ä–æ–±–æ—Ç-–æ–±—Ä–∞–±–æ—Ç—á–∏–∫. –í—ã–¥–∞–≤–∞–π –¢–û–õ–¨–ö–û —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ—Å—Ç–∞. –õ—é–±—ã–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è, –≤–µ–∂–ª–∏–≤–æ—Å—Ç—å –∏ —Ñ—Ä–∞–∑—ã '–í–æ—Ç —Ç–µ–∫—Å—Ç' –ö–ê–¢–ï–ì–û–†–ò–ß–ï–°–ö–ò –ó–ê–ü–†–ï–©–ï–ù–´. –°—Ä–∞–∑—É –Ω–∞—á–∏–Ω–∞–π —Å üî•.\n\n"
        f"–ù–û–í–û–°–¢–¨:\n"
        f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n"
        f"–ö–æ–Ω—Ç–µ–Ω—Ç: {content[:1500]}\n\n"
        f"–°–¢–†–£–ö–¢–£–†–ê:\n"
        f"üî• **[–ñ–∏—Ä–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫]**\n\n"
        f"[–°—É—Ç—å –≤ 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö]\n\n"
        f"‚Ä¢ [–§–∞–∫—Ç 1]\n"
        f"‚Ä¢ [–§–∞–∫—Ç 2]\n\n"
        f"üí° [–°–æ–≤–µ—Ç –∏–ª–∏ –∏—Ç–æ–≥]\n\n"
        f"#—Ç–µ–≥–∏"
    )
    try:
        with DDGS() as ddgs:
            response = ddgs.chat(prompt, model='gpt-4o-mini')
            text = response.strip()
            
            # –£–¥–∞–ª—è–µ–º –≤—Å—ë, —á—Ç–æ –∏–¥–µ—Ç –¥–æ –ø–µ—Ä–≤–æ–≥–æ —ç–º–æ–¥–∑–∏ –∏–ª–∏ –∂–∏—Ä–Ω–æ–≥–æ —à—Ä–∏—Ñ—Ç–∞ (–æ—Ç—Ä–µ–∑–∞–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è)
            start_index = text.find('üî•')
            if start_index != -1:
                text = text[start_index:]
            
            # –£–¥–∞–ª—è–µ–º —Ñ—Ä–∞–∑—ã-–ø–∞—Ä–∞–∑–∏—Ç—ã, –µ—Å–ª–∏ –æ–Ω–∏ –æ—Å—Ç–∞–ª–∏—Å—å
            text = re.sub(r'^(–í–æ—Ç|–¢–µ–∫—Å—Ç|–í–∞—à|–ö–æ–Ω–µ—á–Ω–æ|–ü–æ—Å—Ç|–†–µ–¥–∞–∫—Ç–æ—Ä).*?[:\n]', '', text, flags=re.IGNORECASE | re.DOTALL).strip()
            
            # –ß–∏—Å—Ç–∏–º Markdown-–º—É—Å–æ—Ä
            text = text.replace('```', '').strip()
            
            return text
    except:
        return f"üî• <b>{title}</b>\n\n{content[:300]}..."

def run():
    url = f"[https://newsapi.org/v2/everything?q=(IT](https://newsapi.org/v2/everything?q=(IT) OR —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ OR –Ω–µ–π—Ä–æ—Å–µ—Ç–∏)&language=ru&sortBy=publishedAt&apiKey={NEWS_API_KEY}"
    try:
        response = requests.get(url)
        articles = response.json().get('articles', [])
    except: return

    posted_data = get_posted_data()
    random.shuffle(articles)

    for art in articles:
        link = art['url']
        title = art['title']
        clean_title = re.sub(r'[^\w\s]', '', title).lower().strip()
        
        if link in posted_data or clean_title in posted_data: continue
        
        raw_text = get_full_article(link)
        content = raw_text if (raw_text and len(raw_text) > 300) else art.get('description', "")
        if not content: continue

        final_post = rewrite_text(title, content)
        if len(final_post) < 100: continue

        caption = f"{final_post}\n\nüóû <b>–ü–æ–¥–ø–∏—à–∏—Å—å –Ω–∞ <a href='[https://t.me/SUP_V_BotK](https://t.me/SUP_V_BotK)'>SUP_V_BotK</a></b>"
        
        try:
            if art.get('urlToImage'):
                bot.send_photo(CHANNEL_ID, art['urlToImage'], caption=caption, parse_mode='HTML')
            else:
                bot.send_message(CHANNEL_ID, caption, parse_mode='HTML')
            save_posted_data(link, title)
            break
        except:
            continue

if __name__ == "__main__":
    run()
