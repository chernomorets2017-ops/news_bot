import os
import telebot
import requests
import re
import random
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS

BOT_TOKEN = "8546746980:AAF3z5K85WaBMC-SKTSTN5Tx_dXxXyZXIoQ"
NEWS_API_KEY = "E16b35592a2147989d80d46457d4f916" 
CHANNEL_ID = "@SUP_V_BotK"
DB_FILE = "last_links.txt"

bot = telebot.TeleBot(BOT_TOKEN)

def get_posted_data():
    if not os.path.exists(DB_FILE): return set()
    with open(DB_FILE, "r", encoding="utf-8") as f:
        return set(f.read().splitlines())

def save_posted_data(link, title):
    clean_title = re.sub(r'[^\w\s]', '', title).lower().strip()
    with open(DB_FILE, "a", encoding="utf-8") as f:
        f.write(f"{link}\n{clean_title}\n")

def get_full_article(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        for s in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'form']): s.decompose()
        
        paragraphs = [p.get_text().strip() for p in soup.find_all('p') if len(p.get_text()) > 40]
        text = " ".join(paragraphs)
        
        # –í—ã—Ä–µ–∑–∞–µ–º –º—É—Å–æ—Ä –∏ —Ç–µ—Ö-–∏–Ω—Ñ—É
        text = re.sub(r'(Copyright|¬©|–ê–¥—Ä–µ—Å –¥–ª—è —Å–≤—è–∑–∏|info@|–í—ã–¥–µ–ª–∏—Ç–µ —Ç–µ–∫—Å—Ç|–û—à–∏–±–∫–∞|–ü–æ–∏—Å–∫ –ø–æ —Å–∞–π—Ç—É).*', '', text, flags=re.IGNORECASE)
        return text[:2500]
    except:
        return None

def rewrite_text(title, content):
    prompt = (
        f"–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π. –°–¥–µ–ª–∞–π –ø–æ—Å—Ç –≤ —Å—Ç–∏–ª–µ '–∫–∞—Ä—Ç–æ—á–∫–∏'.\n\n"
        f"–¢–ï–ú–ê: {title}\n"
        f"–ò–ù–§–û: {content}\n\n"
        f"–°–¢–†–£–ö–¢–£–†–ê –ü–û–°–¢–ê:\n"
        f"1. ‚ö°Ô∏è –ñ–∏—Ä–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: –æ—Ç—Ä–∞–∑–∏ –≥–ª–∞–≤–Ω—É—é —Å—É—Ç—å.\n\n"
        f"2. –ö–æ—Ä–æ—Ç–∫–∏–π –∞–±–∑–∞—Ü (2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è): —Ä–∞–∑—ä—è—Å–Ω–∏, —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ.\n\n"
        f"3. –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Ü–∏—Ñ—Ä –∏–ª–∏ —Ñ–∞–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ –±—É–ª–ª–∏—Ç—ã (‚Ä¢).\n\n"
        f"4. –ë–ª–æ–∫ '–ì–ª–∞–≤–Ω–æ–µ –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è': –¥–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Å–æ–≤–µ—Ç –∏–ª–∏ –≤—ã–≤–æ–¥.\n\n"
        f"5. –í –∫–æ–Ω—Ü–µ –¥–æ–±–∞–≤—å 3 —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ö–µ—à—Ç–µ–≥–∞.\n\n"
        f"–ó–ê–ü–†–ï–¢: –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Ñ—Ä–∞–∑—ã '–≤ —Ç–µ–∫—Å—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—Å—è', '–∫–æ—Ä–æ—Ç–∫–æ: –≤–∞–∂–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ'. "
        f"–ü–∏—à–∏ —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã. –ú–∞–∫—Å–∏–º—É–º 500 –∑–Ω–∞–∫–æ–≤. –ó–∞–∫–æ–Ω—á–∏ –º—ã—Å–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é."
    )
    try:
        with DDGS() as ddgs:
            response = ddgs.chat(prompt, model='gpt-4o-mini')
            res = response.strip()
            # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å –ò–ò
            res = re.sub(r'^.*?–Ω–æ–≤–æ—Å—Ç–Ω–æ–π –ø–æ—Å—Ç:|^.*?–ø–µ—Ä–µ—Å–∫–∞–∑:', '', res, flags=re.IGNORECASE).strip()
            
            last_mark = max(res.rfind('.'), res.rfind('!'), res.rfind('?'))
            if last_mark != -1: res = res[:last_mark + 1]
            return res
    except:
        return None

def run():
    url = f"https://newsapi.org/v2/everything?q=(IT OR –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ OR —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ OR –≤—ã–ø–ª–∞—Ç—ã OR –∑–∞–∫–æ–Ω—ã)&language=ru&sortBy=publishedAt&apiKey={NEWS_API_KEY}"
    try:
        articles = requests.get(url).json().get('articles', [])
    except: return

    posted_data = get_posted_data()
    random.shuffle(articles)

    for art in articles:
        link = art['url']
        title = art['title']
        clean_title = re.sub(r'[^\w\s]', '', title).lower().strip()
        
        if link in posted_data or clean_title in posted_data: continue
        
        raw_text = get_full_article(link)
        content = raw_text if (raw_text and len(raw_text) > 300) else art.get('description', "")
        
        if not content or "Copyright" in content: continue

        final_post = rewrite_text(title, content)
        
        if not final_post or len(final_post) < 150 or "–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ —Å—Ñ–µ—Ä–µ" in final_post:
            continue

        caption = f"{final_post}\n\nüóû <b>–ü–æ–¥–ø–∏—à–∏—Å—å –Ω–∞ <a href='https://t.me/SUP_V_BotK'>SUP_V_BotK</a></b>"
        
        try:
            if art.get('urlToImage'):
                bot.send_photo(CHANNEL_ID, art['urlToImage'], caption=caption, parse_mode='HTML')
            else:
                bot.send_message(CHANNEL_ID, caption, parse_mode='HTML')
            save_posted_data(link, title)
            break
        except: continue

if __name__ == "__main__":
    run()
